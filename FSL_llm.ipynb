{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b:\\Ernst\\Italian-Ladin Translation\\SA MCQA\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "# check current path\n",
    "current_path = os.getcwd()\n",
    "os.chdir(current_path)\n",
    "print(current_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open the SA dataset from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your Hugging Face token\n",
    "import huggingface_hub\n",
    "huggingface_hub.login(\"your_huggingface_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 12511/12511 [00:00<00:00, 70653.90 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>italian</th>\n",
       "      <th>ladin</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Siamo stati qui per 1 notte prima della nostra...</td>\n",
       "      <td>i sun stá chiló por 1 nöt dan na nosta partida...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abbiamo soggiornato per due notti alla fine de...</td>\n",
       "      <td>i sun stá döes nes ala fin de nosta croaziera....</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ho soggiornato nell’hotel Acca Palace per una ...</td>\n",
       "      <td>i sun sté te hotel Acca Palace por ma na nöt c...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Prima volta in questo hotel, è stata un’esperi...</td>\n",
       "      <td>la pröma iada te chësc hotel é stada na esperi...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abbiamo soggiornato in questo hotel in passato...</td>\n",
       "      <td>i sun sté te chësc hotel denant y i ne se aspe...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             italian  \\\n",
       "0  Siamo stati qui per 1 notte prima della nostra...   \n",
       "1  Abbiamo soggiornato per due notti alla fine de...   \n",
       "2  Ho soggiornato nell’hotel Acca Palace per una ...   \n",
       "3  Prima volta in questo hotel, è stata un’esperi...   \n",
       "4  Abbiamo soggiornato in questo hotel in passato...   \n",
       "\n",
       "                                               ladin label  \n",
       "0  i sun stá chiló por 1 nöt dan na nosta partida...   pos  \n",
       "1  i sun stá döes nes ala fin de nosta croaziera....   pos  \n",
       "2  i sun sté te hotel Acca Palace por ma na nöt c...   pos  \n",
       "3  la pröma iada te chësc hotel é stada na esperi...   pos  \n",
       "4  i sun sté te chësc hotel denant y i ne se aspe...   pos  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SA_dataset = load_dataset(\"ulinnuha/sentiment_analysis_ladin_italian\")\n",
    "SA_df = pd.DataFrame(SA_dataset[\"train\"])\n",
    "SA_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping dictionary to convert 'pos' to 0 and 'neg' to 1\n",
    "label_map = {'pos': 0, 'neg': 1}\n",
    "\n",
    "# Apply the mapping to the 'label' column\n",
    "SA_df['label'] = SA_df['label'].map(label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data into training and test (For Ladin entries)\n",
    "You can change the column for Italian operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#Set which the language\n",
    "language = 'ladin'\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                  SA_df[language], SA_df['label'],\n",
    "                                  test_size=0.20,\n",
    "                                  random_state=42,\n",
    "                                  stratify = SA_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    7873\n",
       "1    2135\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_SA = pd.DataFrame()\n",
    "data_train_SA['review'] = X_train\n",
    "data_train_SA['label'] = y_train\n",
    "data_train_SA.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    1969\n",
       "1     534\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test_SA = pd.DataFrame()\n",
    "data_test_SA['review'] = X_test\n",
    "data_test_SA['label'] = y_test\n",
    "data_test_SA.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10008, 2503)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train_SA), len(data_test_SA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the SA train and test data to 'dataset' Directory\n",
    "data_train_SA.to_csv(f'dataset/data_train_MCQA_{language}.csv', index=False)\n",
    "data_test_SA.to_csv(f'dataset/data_test_MCQA_{language}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perfom Few-shot Learning using LLM for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the main file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python fsl_main.py \\\n",
    "  --task SA \\\n",
    "  --language ladin \\\n",
    "  --model_name llama_31_70b \\\n",
    "  --dataset_dir ./dataset \\\n",
    "  --batch_size 10 \\\n",
    "  --save_dir ./save_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the prediction results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score\n",
    "\n",
    "# Get the prediction results\n",
    "def get_json_files(task, llm_model, batch_size):\n",
    "    # Define the file prefix file name\n",
    "    file_prefix = (f'{task}_{language}_{llm_model}_size of_{batch_size}_batch_')\n",
    "\n",
    "    save_dir = 'save_results'\n",
    "    matching_files = [f for f in os.listdir(f'{save_dir}') if f.startswith(file_prefix)] #current_path+'/save_results'\n",
    "    # List all files in the directory that start with the specified prefix\n",
    "    # Count the number of matching files\n",
    "    num_files = len(matching_files)\n",
    "    print(f\"Found {num_files} files.\")\n",
    "    batch_start = 0\n",
    "\n",
    "    # Open real data (Ground Truth)\n",
    "    ref_data = pd.read_csv(f'dataset/data_test_{task}_{language}.csv')\n",
    "    num_no_resp = 0\n",
    "    all_scores = []\n",
    "    for i in range(num_files):\n",
    "        # Slicing for the current batch of data\n",
    "        real_data = []\n",
    "        real_data = ref_data.iloc[batch_start:batch_start + batch_size]\n",
    "        batch_start = (i + 1) * batch_size\n",
    "        print(f\"Processing batch {i+1}, starting at index {batch_start}\")\n",
    "        # Open and read the JSON files of translation result\n",
    "        file_loc=os.path.join(save_dir+f'/{file_prefix}{i}.json') #save_dir\n",
    "        print(\"load the json file\", file_loc)\n",
    "        f = open(file_loc, encoding='utf8')\n",
    "        data = json.load(f)\n",
    "        \n",
    "        # if json data is in str, convert to dict\n",
    "        if isinstance(data, str):\n",
    "            data = json.loads(data)\n",
    "        # Ensure 'choices' exists and contains data\n",
    "        if \"choices\" in data and data[\"choices\"]:\n",
    "            data_output = data[\"choices\"][0].get(\"message\", {}).get(\"content\", \"\")\n",
    "            #print(translation_output)\n",
    "            if data_output.strip():  # Check if translation output is not empty\n",
    "                try:\n",
    "                    # Clean the input by removing leading and trailing brackets\n",
    "                    data_output_cleaned = data_output.strip()[1:-1]\n",
    "                    #data_output_cleaned = re.sub(r'\\]\\n*\\[', '], [', data_output_cleaned)\n",
    "                    data_output_cleaned = re.sub(r'\\]\\n*\\[|\\],\\n*\\[|\\], \\\\n\\[', '], [', data_output_cleaned)  \n",
    "\n",
    "                    # Optionally, replace other unwanted patterns, e.g., if there are stray newlines\n",
    "                    data_output_cleaned = data_output_cleaned.replace(\"\\n\", \" \")\n",
    "                    # Split the reviews into separate strings\n",
    "                    predictions = list(map(int, data_output_cleaned.split(', ')))\n",
    "                    labels = real_data['label'].tolist()\n",
    "                    \n",
    "                    if len(predictions) == len(labels):\n",
    "\n",
    "                        # Calculate Balanced Accuracy and F1 Score\n",
    "                        balanced_acc = balanced_accuracy_score(labels, predictions)\n",
    "                        f1 = f1_score(labels, predictions, average='weighted')\n",
    "                        all_scores.append(\n",
    "                                {'ACC': balanced_acc,\n",
    "                                'F1':f1,}\n",
    "                                )\n",
    "                    else:\n",
    "                        print(f\"Length mismatch for batch {i + 1}: {len(predictions)} != {len(real_data)}\")\n",
    "      \n",
    "                except json.JSONDecodeError as ex:\n",
    "                    print(f\"An error occurred while processing the reviews: {ex}\")\n",
    "        else:\n",
    "            print(\"No choices found in the response.\")\n",
    "    print('The number of batches without response of LLM', num_no_resp)\n",
    "    return all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get evaluation metric scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameter for performing Evaluations\n",
    "llm_model = 'llama_31_70b' # gpt/ mt5\n",
    "batch_size = 10\n",
    "task = 'SA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all translation results\n",
    "translation_result=get_json_files(task, llm_model, batch_size)\n",
    "print(len(translation_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n",
      "The Accuracy score for the SA tasks is 0.9791468253968255\n",
      "The F1 score for the SA tasks is 0.9817533302851259\n"
     ]
    }
   ],
   "source": [
    "# Convert the list of dictionaries to a DataFrame\n",
    "filtered_data = [entry for entry in translation_result if not all(isinstance(v, list) and len(v) == 0 for v in entry.values())]\n",
    "fr = pd.DataFrame(filtered_data)\n",
    "print(len(filtered_data))\n",
    "# Calculate the mean for each column\n",
    "mean_scores = fr.mean()\n",
    "# Print the mean scores\n",
    "print(\"The Accuracy score for the SA tasks is\", mean_scores['ACC'])\n",
    "print(\"The F1 score for the SA tasks is\", mean_scores['F1'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. MCQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the dataset from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your Hugging Face token\n",
    "import huggingface_hub\n",
    "huggingface_hub.login(\"your_huggingface_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asus\\miniconda3\\envs\\new_env\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\asus\\.cache\\huggingface\\hub\\datasets--ulinnuha--mcqa_ladin_italian. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating train split: 764 examples [00:00, 9260.77 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_italian</th>\n",
       "      <th>question_ladin</th>\n",
       "      <th>choices_all_italian</th>\n",
       "      <th>choices_all_ladin</th>\n",
       "      <th>max_choices</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lo stato giuridico ed economico del personale ...</td>\n",
       "      <td>le stat juridich y economich dl personal dles ...</td>\n",
       "      <td>['secondo i principi generali e comuni del rap...</td>\n",
       "      <td>['aladô di prinzips generai y comuni dl raport...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Il miocardio è il muscolo:\\n\\n</td>\n",
       "      <td>le miocardium é le muscul:</td>\n",
       "      <td>['del polmone', 'del cuore', 'no, non è un mus...</td>\n",
       "      <td>['dl pulmon', 'de cör', 'no, al né nia n muscul']</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Il mantenimento del pediatra di famiglia fino ...</td>\n",
       "      <td>le mantignimënt dl pediater de familia cina 16...</td>\n",
       "      <td>['se il compimento del sedicesimo anno di età ...</td>\n",
       "      <td>['sce limplenida dl sëdesim ann deté vëgn dant...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cosa sono i batteri?\\n\\n</td>\n",
       "      <td>ci é pa i bacteri?</td>\n",
       "      <td>['Parassiti intracellulari che per potersi rip...</td>\n",
       "      <td>['parassic intracellulars che por podëi se rep...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lo scioglimento della Croce Rossa Italiana può...</td>\n",
       "      <td>la desliagna dla Crusc Röda Italiana pó ester ...</td>\n",
       "      <td>['con D.C.P.M. o con D.P.R.', 'solo con refere...</td>\n",
       "      <td>['cun D.C.P.M. o cun D.P.R.', 'ma cun referend...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    question_italian  \\\n",
       "0  Lo stato giuridico ed economico del personale ...   \n",
       "1                     Il miocardio è il muscolo:\\n\\n   \n",
       "2  Il mantenimento del pediatra di famiglia fino ...   \n",
       "3                           Cosa sono i batteri?\\n\\n   \n",
       "4  Lo scioglimento della Croce Rossa Italiana può...   \n",
       "\n",
       "                                      question_ladin  \\\n",
       "0  le stat juridich y economich dl personal dles ...   \n",
       "1                         le miocardium é le muscul:   \n",
       "2  le mantignimënt dl pediater de familia cina 16...   \n",
       "3                                 ci é pa i bacteri?   \n",
       "4  la desliagna dla Crusc Röda Italiana pó ester ...   \n",
       "\n",
       "                                 choices_all_italian  \\\n",
       "0  ['secondo i principi generali e comuni del rap...   \n",
       "1  ['del polmone', 'del cuore', 'no, non è un mus...   \n",
       "2  ['se il compimento del sedicesimo anno di età ...   \n",
       "3  ['Parassiti intracellulari che per potersi rip...   \n",
       "4  ['con D.C.P.M. o con D.P.R.', 'solo con refere...   \n",
       "\n",
       "                                   choices_all_ladin  max_choices  answer  \n",
       "0  ['aladô di prinzips generai y comuni dl raport...            3       2  \n",
       "1  ['dl pulmon', 'de cör', 'no, al né nia n muscul']            3       1  \n",
       "2  ['sce limplenida dl sëdesim ann deté vëgn dant...            3       2  \n",
       "3  ['parassic intracellulars che por podëi se rep...            3       1  \n",
       "4  ['cun D.C.P.M. o cun D.P.R.', 'ma cun referend...            3       2  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MCQA_dataset = load_dataset(\"ulinnuha/mcqa_ladin_italian\")\n",
    "df_mcqa = pd.DataFrame(MCQA_dataset[\"train\"])\n",
    "df_mcqa.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set which the language\n",
    "language = 'ladin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution in Training Set:\n",
      "max_choices\n",
      "3    0.398361\n",
      "5    0.345902\n",
      "4    0.255738\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Class Distribution in Testing Set:\n",
      "max_choices\n",
      "3    0.396104\n",
      "5    0.344156\n",
      "4    0.259740\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate the proportion of each class in the 'label' column\n",
    "class_proportions = df_mcqa['max_choices'].value_counts(normalize=True)\n",
    "\n",
    "# Create empty DataFrames for the train and test sets\n",
    "df_train = pd.DataFrame()\n",
    "testing_data = pd.DataFrame()\n",
    "\n",
    "# Split the data for each class based on the proportion\n",
    "for label, proportion in class_proportions.items():\n",
    "    # Get all rows for the current class\n",
    "    label_df = df_mcqa[df_mcqa['max_choices'] == label]\n",
    "\n",
    "    # Calculate the number of samples for train and test sets based on class proportion\n",
    "    n_samples = len(label_df)\n",
    "    train_size = int(0.80 * n_samples)  # 80% of samples for training\n",
    "    test_size = n_samples - train_size  # 20% of samples for testing\n",
    "\n",
    "    # Shuffle the rows within this class\n",
    "    label_df_shuffled = label_df.sample(frac=1, random_state=42)\n",
    "\n",
    "    # Split into train and test based on the calculated sizes\n",
    "    label_train = label_df_shuffled.iloc[:train_size]\n",
    "    label_test = label_df_shuffled.iloc[train_size:]\n",
    "\n",
    "    # Append to the corresponding train and test DataFrames\n",
    "    df_train = pd.concat([df_train, label_train], axis=0)\n",
    "    testing_data = pd.concat([testing_data, label_test], axis=0)\n",
    "\n",
    "# Reset indices for better handling\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "testing_data = testing_data.reset_index(drop=True)\n",
    "\n",
    "# Optionally, display the class distribution in both train and test sets\n",
    "print(\"Class Distribution in Training Set:\")\n",
    "print(df_train['max_choices'].value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nClass Distribution in Testing Set:\")\n",
    "print(testing_data['max_choices'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset\n",
    "df_train.to_csv(f'dataset/data_train_MCQA_{language}.csv', index=False)\n",
    "testing_data.to_csv(f'dataset/data_test_MCQA_{language}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Inferencing of LLM using Few-short learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python fsl_main.py \\\n",
    "  --task MCQA \\\n",
    "  --language ladin \\\n",
    "  --model_name llama_31_70b \\\n",
    "  --dataset_dir ./dataset \\\n",
    "  --batch_size 10 \\\n",
    "  --save_dir ./save_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the prediction results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score\n",
    "def get_json_files_mcqa(task, llm_model, batch_size, language):\n",
    "    # Define the file prefix file name\n",
    "    file_prefix = (f'{task}_{language}_{llm_model}_size of_{batch_size}_batch_')\n",
    "\n",
    "    # List all files in the directory that start with the specified prefix\n",
    "    save_dir = 'save_results'\n",
    "    matching_files = [f for f in os.listdir(f'{save_dir}') if f.startswith(file_prefix)] #current_path+'/save_results'\n",
    "    # Count the number of matching files\n",
    "    num_files = len(matching_files)\n",
    "    print(f\"Found {num_files} files.\")\n",
    "    batch_start = 0\n",
    "\n",
    "    # Open real data (Ground Truth)\n",
    "    ref_data = pd.read_csv(f'dataset/data_test_{task}_{language}.csv')\n",
    "    num_no_resp = 0\n",
    "    all_scores = []\n",
    "    for i in range(num_files):\n",
    "        # Slicing for the current batch of data\n",
    "        real_data = []\n",
    "        real_data = ref_data.iloc[batch_start:batch_start + batch_size]\n",
    "        batch_start = (i + 1) * batch_size\n",
    "        print(f\"Processing batch {i+1}, starting at index {batch_start}\")\n",
    "        # Open and read the JSON files of translation result\n",
    "        file_loc=os.path.join(save_dir+f'/{file_prefix}{i}.json') #save_dir\n",
    "        print(\"load the json file\", file_loc)\n",
    "        f = open(file_loc, encoding='utf8')\n",
    "        data = json.load(f)\n",
    "\n",
    "        # if json data is in str, convert to dict\n",
    "        if isinstance(data, str):\n",
    "            data = json.loads(data)\n",
    "        # Ensure 'choices' exists and contains data\n",
    "        if \"choices\" in data and data[\"choices\"]:\n",
    "            data_output = data[\"choices\"][0].get(\"message\", {}).get(\"content\", \"\")\n",
    "            #print(translation_output)\n",
    "            if data_output.strip():  # Check if translation output is not empty\n",
    "                try:\n",
    "                    # Clean the input by removing leading and trailing brackets\n",
    "                    data_output_cleaned = data_output.strip()[1:-1]\n",
    "                    #data_output_cleaned = re.sub(r'\\]\\n*\\[', '], [', data_output_cleaned)\n",
    "                    data_output_cleaned = re.sub(r'\\]\\n*\\[|\\],\\n*\\[|\\], \\\\n\\[', '], [', data_output_cleaned)  \n",
    "\n",
    "                    # Optionally, replace other unwanted patterns, e.g., if there are stray newlines\n",
    "                    data_output_cleaned = data_output_cleaned.replace(\"\\n\", \" \")\n",
    "                    # Split the reviews into separate strings\n",
    "                    predictions = list(map(int, data_output_cleaned.split(', ')))\n",
    "                    labels = real_data['answer'].tolist()\n",
    "                    \n",
    "                    if len(predictions) == len(labels):\n",
    "\n",
    "                        # Calculate Balanced Accuracy and F1 Score\n",
    "                        balanced_acc = balanced_accuracy_score(labels, predictions)\n",
    "                        f1 = f1_score(labels, predictions, average='weighted')\n",
    "                        all_scores.append(\n",
    "                                {'ACC': balanced_acc,\n",
    "                                'F1':f1,}\n",
    "                                )\n",
    "                    else:\n",
    "                        print(f\"Length mismatch for batch {i + 1}: {len(predictions)} != {len(real_data)}\")\n",
    "      \n",
    "                except json.JSONDecodeError as ex:\n",
    "                    print(f\"An error occurred while processing the reviews: {ex}\")\n",
    "        else:\n",
    "            print(\"No choices found in the response.\")\n",
    "    print('The number of batches without response of LLM', num_no_resp)\n",
    "    return all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get evaluation metric scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameter for performing Evaluations\n",
    "llm_model = 'llama_31_70b' # gpt/ mt5\n",
    "batch_size = 10\n",
    "task = 'MCQA' # italian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all translation results\n",
    "translation_result_mcqa=get_json_files_mcqa(task, llm_model, batch_size, language)\n",
    "print(len(translation_result_mcqa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of dictionaries to a DataFrame\n",
    "filtered_data_mcqa = [entry for entry in translation_result_mcqa if not all(isinstance(v, list) and len(v) == 0 for v in entry.values())]\n",
    "fr_mcqa = pd.DataFrame(filtered_data_mcqa)\n",
    "print(len(filtered_data_mcqa))\n",
    "# Calculate the mean for each column\n",
    "mean_scores_mcqa = fr_mcqa.mean()\n",
    "# Print the mean scores\n",
    "print(\"The Accuracy score for the MCQA tasks is\", mean_scores_mcqa['ACC'])\n",
    "print(\"The F1 score for the MCQA tasks is\", mean_scores_mcqa['F1'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
