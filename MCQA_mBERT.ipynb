{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJIv1xVveBNX"
      },
      "outputs": [],
      "source": [
        "# Import the Libraries\n",
        "import torch\n",
        "from transformers import AutoTokenizer, DistilBertForMultipleChoice\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cy7lbAiHeBNY",
        "outputId": "fcdd47c5-9dc7-437c-9818-90da9fa17ef7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Set to initialize the random number generator\n",
        "RANDOM_SEED = 42\n",
        "MAX_LEN = 128\n",
        "BATCH_SIZE=8\n",
        "\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnyvUj-weBNY"
      },
      "source": [
        "## Open the SA dataset from huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XI_kfFHWeBNZ"
      },
      "outputs": [],
      "source": [
        "# Set your Hugging Face token\n",
        "import huggingface_hub\n",
        "from datasets import load_dataset\n",
        "huggingface_hub.login(\"your_huggingface_token\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "HnY1aK0ErDPX",
        "outputId": "6a91ace2-23c8-451d-eba2-a4b5cbeda182"
      },
      "outputs": [],
      "source": [
        "MCQA_dataset = load_dataset(\"ulinnuha/mcqa_ladin_italian\")\n",
        "MCQA_df = pd.DataFrame(MCQA_dataset[\"train\"])\n",
        "MCQA_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set the number of choices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set the Class number\n",
        "num_choices= 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kVmElN86Fpn"
      },
      "outputs": [],
      "source": [
        "MCQA_df = MCQA_df[MCQA_df['max_choices']== num_choices].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UtNcrQkQX5u"
      },
      "source": [
        "### Set the training and testing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krr0OWIfQOho",
        "outputId": "16a38b22-9c54-4e18-f139-4237bfe1942c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class Distribution in Training Set:\n",
            "max_choices\n",
            "5    1.0\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Class Distribution in Testing Set:\n",
            "max_choices\n",
            "5    1.0\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Calculate the proportion of each class in the choice's max number column\n",
        "class_proportions = MCQA_df['max_choices'].value_counts(normalize=True)\n",
        "\n",
        "# Create empty DataFrames for the train and test sets\n",
        "df_train = pd.DataFrame()\n",
        "testing_data = pd.DataFrame()\n",
        "\n",
        "# Split the data for each class based on the proportion\n",
        "for label, proportion in class_proportions.items():\n",
        "    # Get all rows for the current class\n",
        "    label_df = MCQA_df[MCQA_df['max_choices'] == label]\n",
        "\n",
        "    # Calculate the number of samples for train and test sets based on class proportion\n",
        "    n_samples = len(label_df)\n",
        "    train_size = int(0.80 * n_samples)  # 80% of samples for training\n",
        "    test_size = n_samples - train_size  # 20% of samples for testing\n",
        "\n",
        "    # Shuffle the rows within this class\n",
        "    label_df_shuffled = label_df.sample(frac=1, random_state=42)\n",
        "\n",
        "    # Split into train and test based on the calculated sizes\n",
        "    label_train = label_df_shuffled.iloc[:train_size]\n",
        "    label_test = label_df_shuffled.iloc[train_size:]\n",
        "\n",
        "    # Append to the corresponding train and test DataFrames\n",
        "    df_train = pd.concat([df_train, label_train], axis=0)\n",
        "    testing_data = pd.concat([testing_data, label_test], axis=0)\n",
        "\n",
        "# Reset indices for better handling\n",
        "df_train = df_train.reset_index(drop=True)\n",
        "testing_data = testing_data.reset_index(drop=True)\n",
        "\n",
        "# Optionally, display the class distribution in both train and test sets\n",
        "print(\"Class Distribution in Training Set:\")\n",
        "print(df_train['max_choices'].value_counts(normalize=True))\n",
        "\n",
        "print(\"\\nClass Distribution in Testing Set:\")\n",
        "print(testing_data['max_choices'].value_counts(normalize=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZEXyiwDQY_C"
      },
      "source": [
        "### Set the training and validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kF5A22ZQa0C",
        "outputId": "78b51f4c-5d13-4200-c5c4-5cbdfdf365d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class Distribution in Training Set:\n",
            "max_choices\n",
            "5    1.0\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Class Distribution in Testing Set:\n",
            "max_choices\n",
            "5    1.0\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Calculate the proportion of each class in the 'label' column\n",
        "class_proportions = df_train['max_choices'].value_counts(normalize=True)\n",
        "\n",
        "# Create empty DataFrames for the train and test sets\n",
        "training_data = pd.DataFrame()\n",
        "val_data = pd.DataFrame()\n",
        "\n",
        "# Split the data for each class based on the proportion\n",
        "for label, proportion in class_proportions.items():\n",
        "    # Get all rows for the current class\n",
        "    label_df = df_train[df_train['max_choices'] == label]\n",
        "\n",
        "    # Calculate the number of samples for train and test sets based on class proportion\n",
        "    n_samples = len(label_df)\n",
        "    train_size = int(0.90 * n_samples)  # 80% of samples for training\n",
        "    val_size = n_samples - train_size  # 20% of samples for testing\n",
        "\n",
        "    # Shuffle the rows within this class\n",
        "    label_df_shuffled = label_df.sample(frac=1, random_state=42)\n",
        "\n",
        "    # Split into train and test based on the calculated sizes\n",
        "    label_train = label_df_shuffled.iloc[:train_size]\n",
        "    label_val = label_df_shuffled.iloc[train_size:]\n",
        "\n",
        "    # Append to the corresponding train and test DataFrames\n",
        "    training_data = pd.concat([training_data, label_train], axis=0)\n",
        "    val_data = pd.concat([val_data, label_val], axis=0)\n",
        "\n",
        "# Reset indices for better handling\n",
        "training_data = training_data.reset_index(drop=True)\n",
        "val_data = val_data.reset_index(drop=True)\n",
        "\n",
        "# Optionally, display the class distribution in both train and test sets\n",
        "print(\"Class Distribution in Training Set:\")\n",
        "print(training_data['max_choices'].value_counts(normalize=True))\n",
        "\n",
        "print(\"\\nClass Distribution in Testing Set:\")\n",
        "print(val_data['max_choices'].value_counts(normalize=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-multilingual-cased\")\n",
        "model = DistilBertForMultipleChoice.from_pretrained('distilbert-base-multilingual-cased', num_labels=num_choices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Set which the language for MCQA task\n",
        "language = 'ladin'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpAQkVClIY9o"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOpBTVUQnEWS"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "# Data Loader function\n",
        "def CreateDataloader(df, language):\n",
        "    encodings = []\n",
        "    labels = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        question = row[f'question_{language}']\n",
        "        raw_choices = row[f'choices_all_{language}']\n",
        "\n",
        "        # Handle cases where `raw_choices` is already a list\n",
        "        if isinstance(raw_choices, list):\n",
        "            choices = raw_choices\n",
        "        elif isinstance(raw_choices, str):\n",
        "            try:\n",
        "                choices = eval(raw_choices)  # Convert string representation to list\n",
        "                if not isinstance(choices, list):\n",
        "                    raise ValueError(\"Extracted choices are not a list\")\n",
        "            except:\n",
        "                print(f\"Skipping row {idx} due to invalid choices format: {raw_choices}\")\n",
        "                continue\n",
        "        else:\n",
        "            print(f\"Skipping row {idx} due to unknown format: {type(raw_choices)}\")\n",
        "            continue\n",
        "\n",
        "        correct_answer_idx = row['answer']\n",
        "\n",
        "        if correct_answer_idx >= len(choices):\n",
        "            print(f\"Skipping row {idx} due to index error in answer column.\")\n",
        "            continue\n",
        "\n",
        "        # Tokenize each question-choice pair separately\n",
        "        example_encodings = []\n",
        "        for choice in choices:\n",
        "            # For each choice, encode the [CLS] question [SEP] choice pair\n",
        "            encoding = tokenizer.encode(\n",
        "                f\"[CLS] {question} [SEP] {choice}\",\n",
        "                add_special_tokens=True,\n",
        "                max_length=MAX_LEN,\n",
        "                padding=\"max_length\",\n",
        "                truncation=True\n",
        "            )\n",
        "            example_encodings.append(encoding)\n",
        "\n",
        "        # Stack encodings for each choice into one batch entry \n",
        "        encodings.append(example_encodings)\n",
        "        labels.append(correct_answer_idx)\n",
        "\n",
        "    # Convert list of encodings to a tensor with shape \n",
        "    input_ids = torch.tensor(encodings)  # Shape: \n",
        "\n",
        "    # Create attention mask (1 for real tokens, 0 for padding)\n",
        "    input_mask_array = []\n",
        "    for example in encodings:\n",
        "        example_attention_mask = []\n",
        "        for sent in example:\n",
        "            att_mask = [int(token_id > 0) for token_id in sent]  # 1 for non-padding, 0 for padding\n",
        "            example_attention_mask.append(att_mask)\n",
        "        input_mask_array.append(example_attention_mask)\n",
        "\n",
        "    # Convert attention mask to tensor\n",
        "    input_mask_array = torch.tensor(input_mask_array)\n",
        "\n",
        "    # Convert labels to tensor (shape: (batch_size,))\n",
        "    label_id_array = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    # Building the TensorDataset\n",
        "    dataset = TensorDataset(input_ids, input_mask_array, label_id_array)\n",
        "\n",
        "    return DataLoader(\n",
        "        dataset,  # The training samples.\n",
        "        sampler=RandomSampler(dataset),\n",
        "        pin_memory=True,\n",
        "        batch_size=BATCH_SIZE\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPBv7wpjP4UY"
      },
      "outputs": [],
      "source": [
        "# Create Data loader on training, validation and testing data\n",
        "train_dataloader = CreateDataloader(training_data, language)\n",
        "val_dataloader = CreateDataloader(val_data, language)\n",
        "test_dataloader = CreateDataloader(testing_data, language)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNihGTqwP0kv"
      },
      "source": [
        "# Training Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPdwjPhFN3Up",
        "outputId": "e7a4b307-b763-4a24-acd8-f58ed9073621"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Batch     0  of     24. \n",
            "  Batch     1  of     24. \n",
            "  Batch     2  of     24. \n",
            "  Batch     3  of     24. \n",
            "  Batch     4  of     24. \n",
            "  Batch     5  of     24. \n",
            "  Batch     6  of     24. \n",
            "  Batch     7  of     24. \n",
            "  Batch     8  of     24. \n",
            "  Batch     9  of     24. \n",
            "  Batch    10  of     24. \n",
            "  Batch    11  of     24. \n",
            "  Batch    12  of     24. \n",
            "  Batch    13  of     24. \n",
            "  Batch    14  of     24. \n",
            "  Batch    15  of     24. \n",
            "  Batch    16  of     24. \n",
            "  Batch    17  of     24. \n",
            "  Batch    18  of     24. \n",
            "  Batch    19  of     24. \n",
            "  Batch    20  of     24. \n",
            "  Batch    21  of     24. \n",
            "  Batch    22  of     24. \n",
            "  Batch    23  of     24. \n",
            "Epoch 1/5 - Train Loss: 38.4895\n",
            "Train Balanced Accuracy: 0.1562\n",
            "Train F1 Score: 0.1600\n",
            "Epoch 1/5 - Validation Loss: 4.8366\n",
            "Validation Balanced Accuracy: 0.1638\n",
            "Validation F1 Score: 0.1887\n",
            "  Batch     0  of     24. \n",
            "  Batch     1  of     24. \n",
            "  Batch     2  of     24. \n",
            "  Batch     3  of     24. \n",
            "  Batch     4  of     24. \n",
            "  Batch     5  of     24. \n",
            "  Batch     6  of     24. \n",
            "  Batch     7  of     24. \n",
            "  Batch     8  of     24. \n",
            "  Batch     9  of     24. \n",
            "  Batch    10  of     24. \n",
            "  Batch    11  of     24. \n",
            "  Batch    12  of     24. \n",
            "  Batch    13  of     24. \n",
            "  Batch    14  of     24. \n",
            "  Batch    15  of     24. \n",
            "  Batch    16  of     24. \n",
            "  Batch    17  of     24. \n",
            "  Batch    18  of     24. \n",
            "  Batch    19  of     24. \n",
            "  Batch    20  of     24. \n",
            "  Batch    21  of     24. \n",
            "  Batch    22  of     24. \n",
            "  Batch    23  of     24. \n",
            "Epoch 2/5 - Train Loss: 38.4951\n",
            "Train Balanced Accuracy: 0.1937\n",
            "Train F1 Score: 0.1863\n",
            "Epoch 2/5 - Validation Loss: 4.8393\n",
            "Validation Balanced Accuracy: 0.0686\n",
            "Validation F1 Score: 0.1023\n",
            "  Batch     0  of     24. \n",
            "  Batch     1  of     24. \n",
            "  Batch     2  of     24. \n",
            "  Batch     3  of     24. \n",
            "  Batch     4  of     24. \n",
            "  Batch     5  of     24. \n",
            "  Batch     6  of     24. \n",
            "  Batch     7  of     24. \n",
            "  Batch     8  of     24. \n",
            "  Batch     9  of     24. \n",
            "  Batch    10  of     24. \n",
            "  Batch    11  of     24. \n",
            "  Batch    12  of     24. \n",
            "  Batch    13  of     24. \n",
            "  Batch    14  of     24. \n",
            "  Batch    15  of     24. \n",
            "  Batch    16  of     24. \n",
            "  Batch    17  of     24. \n",
            "  Batch    18  of     24. \n",
            "  Batch    19  of     24. \n",
            "  Batch    20  of     24. \n",
            "  Batch    21  of     24. \n",
            "  Batch    22  of     24. \n",
            "  Batch    23  of     24. \n",
            "Epoch 3/5 - Train Loss: 37.3906\n",
            "Train Balanced Accuracy: 0.3234\n",
            "Train F1 Score: 0.3194\n",
            "Epoch 3/5 - Validation Loss: 5.0086\n",
            "Validation Balanced Accuracy: 0.1852\n",
            "Validation F1 Score: 0.1846\n",
            "  Batch     0  of     24. \n",
            "  Batch     1  of     24. \n",
            "  Batch     2  of     24. \n",
            "  Batch     3  of     24. \n",
            "  Batch     4  of     24. \n",
            "  Batch     5  of     24. \n",
            "  Batch     6  of     24. \n",
            "  Batch     7  of     24. \n",
            "  Batch     8  of     24. \n",
            "  Batch     9  of     24. \n",
            "  Batch    10  of     24. \n",
            "  Batch    11  of     24. \n",
            "  Batch    12  of     24. \n",
            "  Batch    13  of     24. \n",
            "  Batch    14  of     24. \n",
            "  Batch    15  of     24. \n",
            "  Batch    16  of     24. \n",
            "  Batch    17  of     24. \n",
            "  Batch    18  of     24. \n",
            "  Batch    19  of     24. \n",
            "  Batch    20  of     24. \n",
            "  Batch    21  of     24. \n",
            "  Batch    22  of     24. \n",
            "  Batch    23  of     24. \n",
            "Epoch 4/5 - Train Loss: 33.5601\n",
            "Train Balanced Accuracy: 0.4186\n",
            "Train F1 Score: 0.4190\n",
            "Epoch 4/5 - Validation Loss: 5.0295\n",
            "Validation Balanced Accuracy: 0.2352\n",
            "Validation F1 Score: 0.2194\n",
            "  Batch     0  of     24. \n",
            "  Batch     1  of     24. \n",
            "  Batch     2  of     24. \n",
            "  Batch     3  of     24. \n",
            "  Batch     4  of     24. \n",
            "  Batch     5  of     24. \n",
            "  Batch     6  of     24. \n",
            "  Batch     7  of     24. \n",
            "  Batch     8  of     24. \n",
            "  Batch     9  of     24. \n",
            "  Batch    10  of     24. \n",
            "  Batch    11  of     24. \n",
            "  Batch    12  of     24. \n",
            "  Batch    13  of     24. \n",
            "  Batch    14  of     24. \n",
            "  Batch    15  of     24. \n",
            "  Batch    16  of     24. \n",
            "  Batch    17  of     24. \n",
            "  Batch    18  of     24. \n",
            "  Batch    19  of     24. \n",
            "  Batch    20  of     24. \n",
            "  Batch    21  of     24. \n",
            "  Batch    22  of     24. \n",
            "  Batch    23  of     24. \n",
            "Epoch 5/5 - Train Loss: 30.1609\n",
            "Train Balanced Accuracy: 0.4847\n",
            "Train F1 Score: 0.4786\n",
            "Epoch 5/5 - Validation Loss: 4.9114\n",
            "Validation Balanced Accuracy: 0.0686\n",
            "Validation F1 Score: 0.0928\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import balanced_accuracy_score, f1_score\n",
        "import torch\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Number of epochs\n",
        "num_epochs = 5  # Example number of epochs\n",
        "# Training and validation loop\n",
        "for epoch in range(num_epochs):\n",
        "    # Training Phase\n",
        "    model.train().to(device)   # Set the model to training mode\n",
        "    total_loss_train = 0\n",
        "    train_predictions = []\n",
        "    train_true_labels = []\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # Progress update every print_each_n_step batches.\n",
        "        print('  Batch {:>5,}  of  {:>5,}. '.format(step, len(train_dataloader)))\n",
        "\n",
        "        # Unpack this training batch from the dataloader\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Get the predicted choice of each entry using mBRET in the training stage\n",
        "        outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
        "\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss_train += loss.item()\n",
        "\n",
        "        # Get predictions and append to the list\n",
        "        predicted_idx = torch.argmax(logits, dim=1).cpu().numpy()  # Move to CPU for numpy operations\n",
        "        train_predictions.extend(predicted_idx)\n",
        "\n",
        "        # Move b_labels to CPU and then convert to NumPy\n",
        "        b_labels_cpu = b_labels.cpu().numpy()  # Ensure it's a NumPy array\n",
        "        train_true_labels.extend(b_labels_cpu)  # Append the labels\n",
        "\n",
        "        # Debugging: Check lengths after each batch\n",
        "        #print(f\"Batch {step} - train_true_labels length: {len(train_true_labels)}, train_predictions length: {len(train_predictions)}\")\n",
        "\n",
        "    # Calculate training metrics after processing all batches in an epoch\n",
        "    train_balanced_accuracy = balanced_accuracy_score(train_true_labels, train_predictions)\n",
        "    train_f1_score = f1_score(train_true_labels, train_predictions, average='weighted')\n",
        "\n",
        "    # Print training statistics for the epoch\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {total_loss_train:.4f}\")\n",
        "    print(f\"Train Balanced Accuracy: {train_balanced_accuracy:.4f}\")\n",
        "    print(f\"Train F1 Score: {train_f1_score:.4f}\")\n",
        "\n",
        "    # Validation Phase\n",
        "    model.eval().to(device)   # Set the model to evaluation mode\n",
        "    total_loss_val = 0\n",
        "    val_predictions = []\n",
        "    val_true_labels = []\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation for validation\n",
        "        for step, batch in enumerate(val_dataloader):\n",
        "            # Unpack this validation batch from the dataloader\n",
        "            b_input_ids = batch[0].to(device)\n",
        "            b_input_mask = batch[1].to(device)\n",
        "            b_labels = batch[2].to(device)\n",
        "\n",
        "            # Get the contextual representation using Transformer model\n",
        "            outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
        "\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "            total_loss_val += loss.item()\n",
        "\n",
        "            # Get predictions and append to the list\n",
        "            predicted_idx = torch.argmax(logits, dim=1).cpu().numpy()  # Move to CPU for numpy operations\n",
        "            val_predictions.extend(predicted_idx)\n",
        "\n",
        "            # Move b_labels to CPU and then convert to NumPy\n",
        "            b_labels_cpu = b_labels.cpu().numpy()  # Ensure it's a NumPy array\n",
        "            val_true_labels.extend(b_labels_cpu)  # Append the labels\n",
        "\n",
        "        # Calculate validation metrics after processing all batches in an epoch\n",
        "        val_balanced_accuracy = balanced_accuracy_score(val_true_labels, val_predictions)\n",
        "        val_f1_score = f1_score(val_true_labels, val_predictions, average='weighted')\n",
        "\n",
        "    # Print validation statistics for the epoch\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Validation Loss: {total_loss_val:.4f}\")\n",
        "    print(f\"Validation Balanced Accuracy: {val_balanced_accuracy:.4f}\")\n",
        "    print(f\"Validation F1 Score: {val_f1_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing stage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQzGey5fGXbY",
        "outputId": "1b832a16-b01c-4606-b026-3f494a0a0713"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Test Batch 1 of 7.\n",
            "  Test Batch 2 of 7.\n",
            "  Test Batch 3 of 7.\n",
            "  Test Batch 4 of 7.\n",
            "  Test Batch 5 of 7.\n",
            "  Test Batch 6 of 7.\n",
            "  Test Batch 7 of 7.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import balanced_accuracy_score, f1_score\n",
        "import torch\n",
        "\n",
        "# Initialize lists to hold predictions and true labels for testing\n",
        "test_predictions = []\n",
        "test_true_labels = []\n",
        "\n",
        "# Set the model to evaluation mode for testing (this disables dropout layers, etc.)\n",
        "model.eval().to(device)\n",
        "\n",
        "# Disable gradient computation during evaluation (this speeds up the process)\n",
        "with torch.no_grad():\n",
        "    total_loss_test = 0\n",
        "    for step, batch in enumerate(test_dataloader):\n",
        "        print(f\"  Test Batch {step+1} of {len(test_dataloader)}.\")\n",
        "\n",
        "        # Unpack the test batch from the dataloader\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Get the model's output (logits and loss)\n",
        "        outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
        "\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        total_loss_test += loss.item()\n",
        "\n",
        "        # Get predictions and append to the list\n",
        "        predicted_idx = torch.argmax(logits, dim=1).cpu().numpy()  # Move to CPU for numpy operations\n",
        "        test_predictions.extend(predicted_idx)\n",
        "\n",
        "        # Move b_labels to CPU and then convert to NumPy\n",
        "        b_labels_cpu = b_labels.cpu().numpy()  # Ensure it's a NumPy array\n",
        "        test_true_labels.extend(b_labels_cpu)  # Append the labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Calculate the evaluation metrics in  testing strage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GgeasD2GZDq",
        "outputId": "e34b2fab-66a2-4a5f-db53-915a5aaa56b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 11.0647\n",
            "Test Balanced Accuracy: 0.2206\n",
            "Test F1 Score: 0.2265\n"
          ]
        }
      ],
      "source": [
        "test_balanced_accuracy = balanced_accuracy_score(test_true_labels, test_predictions)\n",
        "test_f1_score = f1_score(test_true_labels, test_predictions, average='weighted')\n",
        "\n",
        "# Print test statistics\n",
        "print(f\"Test Loss: {total_loss_test:.4f}\")\n",
        "print(f\"Test Balanced Accuracy: {test_balanced_accuracy:.4f}\")\n",
        "print(f\"Test F1 Score: {test_f1_score:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "new_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
